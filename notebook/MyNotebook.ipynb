{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "savefile = \"./STORED_model/my_trained_model.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to help intialize random weights for fully connected or convolutional layers, we leave the shape attribute as a parameter for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    init_random_dist = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(init_random_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as init_weights, but for the biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_bias(shape):\n",
    "    init_bias_vals = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(init_bias_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a 2D convolution using builtin conv2d from TF. From those docs:\n",
    "\n",
    "Computes a 2-D convolution given 4-D `input` and `filter` tensors.\n",
    "\n",
    "Given an input tensor of shape `[batch, in_height, in_width, in_channels]`\n",
    "and a filter / kernel tensor of shape\n",
    "`[filter_height, filter_width, in_channels, out_channels]`, this op\n",
    "performs the following:\n",
    "\n",
    "1. Flattens the filter to a 2-D matrix with shape\n",
    "   `[filter_height * filter_width * in_channels, output_channels]`.\n",
    "2. Extracts image patches from the input tensor to form a *virtual*\n",
    "   tensor of shape `[batch, out_height, out_width,\n",
    "   filter_height * filter_width * in_channels]`.\n",
    "3. For each patch, right-multiplies the filter matrix and the image patch\n",
    "   vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a max pooling layer, again using built in TF functions:\n",
    "\n",
    "Performs the max pooling on the input.\n",
    "\n",
    "    Args:\n",
    "      value: A 4-D `Tensor` with shape `[batch, height, width, channels]` and\n",
    "        type `tf.float32`.\n",
    "      ksize: A list of ints that has length >= 4.  The size of the window for\n",
    "        each dimension of the input tensor.\n",
    "      strides: A list of ints that has length >= 4.  The stride of the sliding\n",
    "        window for each dimension of the input tensor.\n",
    "      padding: A string, either `'VALID'` or `'SAME'`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_pool_2by2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the conv2d function, we'll return an actual convolutional layer here that uses an ReLu activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convolutional_layer(input_x, shape):\n",
    "    W = init_weights(shape)\n",
    "    b = init_bias([shape[3]])\n",
    "    return tf.nn.relu(conv2d(input_x, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a normal fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normal_full_layer(input_layer, size, softact=False):\n",
    "    input_size = int(input_layer.get_shape()[1])\n",
    "    W = init_weights([input_size, size])\n",
    "    b = init_bias([size])\n",
    "    if softact:\n",
    "        return tf.nn.softmax(tf.matmul(input_layer, W) + b)\n",
    "    else:\n",
    "        return tf.matmul(input_layer, W) + b\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper (custom functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(pos):\n",
    "    '''\n",
    "    For use to one-hot encode the 10- possible labels\n",
    "    '''\n",
    "    out = np.zeros(10)\n",
    "    out[pos] = 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smile lenght 124386\n",
      "car lenght 182764\n",
      "pencil lenght 122001\n",
      "burger lenght 129672\n",
      "moon lenght 121661\n",
      "hand lenght 291773\n",
      "tornado lenght 143271\n",
      "scissors lenght 123390\n",
      "mug lenght 152918\n",
      "sock lenght 205715\n"
     ]
    }
   ],
   "source": [
    "#duck smile car pencil star burger cookie rabbit moon icecream\n",
    "fileList = ['smile','car','pencil','burger','moon','hand','tornado','scissors','mug','sock']\n",
    "for i in range(len(fileList)):\n",
    "    print('{} lenght {}'.format(fileList[i], len(np.load('./SKETCH_data/'+fileList[i]+'.npy'))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display(img, label, predict):\n",
    "    plt.title('Real %s. Predict: %s - %s' % (label, predict, \"Correct\" if (label==predict) else \"No correct\" ))\n",
    "    plt.imshow(img.reshape((28,28)), cmap=plt.cm.gray_r)\n",
    "    plt.show()\n",
    "    \n",
    "# usage: display(test_x[0], 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SketchImageHelper():\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"Init SketchImageHelper\")\n",
    "        self.position = 0\n",
    "        \n",
    "        self.batch_x = None\n",
    "        self.batch_y = None\n",
    "        \n",
    "        self.pos_begin = 1000\n",
    "        self.pos_end = 110000\n",
    "        \n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        self.fileList = fileList\n",
    "    \n",
    "    def set_up_images(self):\n",
    "        \n",
    "        print(\"Setting Up Batch Images and Labels\")\n",
    "        sampleSize = self.pos_end - self.pos_begin\n",
    "        i = 0\n",
    "        for i in range(len(self.fileList)):\n",
    "            partialImages = np.array(np.load('./SKETCH_data/'+ self.fileList[i] +'.npy')[self.pos_begin:self.pos_end])\n",
    "            self.images.append( partialImages / 255)\n",
    "            self.labels.append(np.full((sampleSize,10), one_hot_encode(i)))\n",
    "\n",
    "        print('batch lenght {}'.format(len(self.images)))\n",
    "        print('batch lenght {}'.format(len(self.labels)))\n",
    "        \n",
    "        \n",
    "    def next_batch(self, batch_size):                          \n",
    "        x = []\n",
    "        y = []\n",
    "        partial_batch = batch_size // len(self.fileList)\n",
    "        i = 0\n",
    "        for i in range(len(self.fileList)):\n",
    "            if i==0:\n",
    "                x = np.array((self.images[i])[self.position:self.position+partial_batch])\n",
    "                y = np.array((self.labels[i])[self.position:self.position+partial_batch])\n",
    "            else:\n",
    "                x = np.concatenate((x,np.array((self.images[i])[self.position:self.position+partial_batch])), axis=0)\n",
    "                y = np.concatenate((y,np.array((self.labels[i])[self.position:self.position+partial_batch])), axis=0)  \n",
    "\n",
    "        \n",
    "        self.position = (self.position + partial_batch)\n",
    "        print(' {}'.format(self.position), end='')\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32,shape=[None,784])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_true = tf.placeholder(tf.float32,shape=[None,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_image = tf.reshape(x,[-1,28,28,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using a 6by6 filter here, used 5by5 in video, you can play around with the filter size\n",
    "# You can change the 32 output, that essentially represents the amount of filters used\n",
    "# You need to pass in 32 to the next input though, the 1 comes from the original input of \n",
    "# a single image.\n",
    "convo_1 = convolutional_layer(x_image,shape=[6,6,1,32])\n",
    "convo_1_pooling = max_pool_2by2(convo_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using a 6by6 filter here, used 5by5 in video, you can play around with the filter size\n",
    "# You can actually change the 64 output if you want, you can think of that as a representation\n",
    "# of the amount of 6by6 filters used.\n",
    "convo_2 = convolutional_layer(convo_1_pooling,shape=[6,6,32,64])\n",
    "convo_2_pooling = max_pool_2by2(convo_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Why 7 by 7 image? Because we did 2 pooling layers, so (28/2)/2 = 7\n",
    "# 64 then just comes from the output of the previous Convolution\n",
    "convo_2_flat = tf.reshape(convo_2_pooling,[-1,7*7*64])\n",
    "full_layer_one = tf.nn.relu(normal_full_layer(convo_2_flat,1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTE THE PLACEHOLDER HERE!\n",
    "hold_prob = tf.placeholder(tf.float32)\n",
    "full_one_dropout = tf.nn.dropout(full_layer_one,keep_prob=hold_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = normal_full_layer(full_one_dropout,10,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true,logits=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.0001) # 0.0001\n",
    "train = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intialize Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_x lenght 5000\n",
      "test_y lenght 5000\n"
     ]
    }
   ],
   "source": [
    "#duck smile car pencil star burger cookie rabbit moon icecream\n",
    "\n",
    "pos_begin = 0\n",
    "pos_end = 500\n",
    "test_x = np.concatenate((np.array(np.load('./SKETCH_data/smile.npy')[pos_begin:pos_end]),np.array(np.load('./SKETCH_data/car.npy')[pos_begin:pos_end]),np.array(np.load('./SKETCH_data/pencil.npy')[pos_begin:pos_end]),np.array(np.load('./SKETCH_data/burger.npy')[pos_begin:pos_end]),np.array(np.load('./SKETCH_data/moon.npy')[pos_begin:pos_end]),np.array(np.load('./SKETCH_data/hand.npy')[pos_begin:pos_end]),np.array(np.load('./SKETCH_data/tornado.npy')[pos_begin:pos_end]),np.array(np.load('./SKETCH_data/scissors.npy')[pos_begin:pos_end]),np.array(np.load('./SKETCH_data/mug.npy')[pos_begin:pos_end]),np.array(np.load('./SKETCH_data/sock.npy')[pos_begin:pos_end])), axis=0)\n",
    "test_y = np.concatenate((np.full((pos_end-pos_begin,10), one_hot_encode(0)), np.full((pos_end-pos_begin,10), one_hot_encode(1)), np.full((pos_end-pos_begin,10), one_hot_encode(2)), np.full((pos_end-pos_begin,10), one_hot_encode(3)),\n",
    "                               np.full((pos_end-pos_begin,10), one_hot_encode(4)), np.full((pos_end-pos_begin,10), one_hot_encode(5)), np.full((pos_end-pos_begin,10), one_hot_encode(6)), np.full((pos_end-pos_begin,10), one_hot_encode(7)),\n",
    "                               np.full((pos_end-pos_begin,10), one_hot_encode(8)), np.full((pos_end-pos_begin,10), one_hot_encode(9))), axis=0)\n",
    "\n",
    "print('test_x lenght {}'.format(len(test_x)))\n",
    "print('test_y lenght {}'.format(len(test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init SketchImageHelper\n",
      "Setting Up Batch Images and Labels\n",
      "batch lenght 10\n",
      "batch lenght 10\n"
     ]
    }
   ],
   "source": [
    "sih = SketchImageHelper()\n",
    "sih.set_up_images()\n",
    "saver = tf.train.Saver()\n",
    "#sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INIT\n",
      " 10\n",
      "\n",
      "step 0\n",
      "Accuracy is:\n",
      "0.1168\n",
      " 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510\n",
      "\n",
      "step 50\n",
      "Accuracy is:\n",
      "0.304\n",
      " 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 790 800 810 820 830 840 850 860 870 880 890 900 910 920 930 940 950 960 970 980 990 1000 1010\n",
      "\n",
      "step 100\n",
      "Accuracy is:\n",
      "0.4342\n",
      " 1020 1030 1040 1050 1060 1070 1080 1090 1100 1110 1120 1130 1140 1150 1160 1170 1180 1190 1200 1210 1220 1230 1240 1250 1260 1270 1280 1290 1300 1310 1320 1330 1340 1350 1360 1370 1380 1390 1400 1410 1420 1430 1440 1450 1460 1470 1480 1490 1500 1510\n",
      "\n",
      "step 150\n",
      "Accuracy is:\n",
      "0.5092\n",
      " 1520 1530 1540 1550 1560 1570 1580 1590 1600 1610 1620 1630 1640 1650 1660 1670 1680 1690 1700 1710 1720 1730 1740 1750 1760 1770 1780 1790 1800 1810 1820 1830 1840 1850 1860 1870 1880 1890 1900 1910 1920 1930 1940 1950 1960 1970 1980 1990 2000 2010\n",
      "\n",
      "step 200\n",
      "Accuracy is:\n",
      "0.5832\n",
      " 2020 2030 2040 2050 2060 2070 2080 2090 2100 2110 2120 2130 2140 2150 2160 2170 2180 2190 2200 2210 2220 2230 2240 2250 2260 2270 2280 2290 2300 2310 2320 2330 2340 2350 2360 2370 2380 2390 2400 2410 2420 2430 2440 2450 2460 2470 2480 2490 2500 2510\n",
      "\n",
      "step 250\n",
      "Accuracy is:\n",
      "0.6586\n",
      " 2520 2530 2540 2550 2560 2570 2580 2590 2600 2610 2620 2630 2640 2650 2660 2670 2680 2690 2700 2710 2720 2730 2740 2750 2760 2770 2780 2790 2800 2810 2820 2830 2840 2850 2860 2870 2880 2890 2900 2910 2920 2930 2940 2950 2960 2970 2980 2990 3000 3010\n",
      "\n",
      "step 300\n",
      "Accuracy is:\n",
      "0.7124\n",
      " 3020 3030 3040 3050 3060 3070 3080 3090 3100 3110 3120 3130 3140 3150 3160 3170 3180 3190 3200 3210 3220 3230 3240 3250 3260 3270 3280 3290 3300 3310 3320 3330 3340 3350 3360 3370 3380 3390 3400 3410 3420 3430 3440 3450 3460 3470 3480 3490 3500 3510\n",
      "\n",
      "step 350\n",
      "Accuracy is:\n",
      "0.719\n",
      " 3520 3530 3540 3550 3560 3570 3580 3590 3600 3610 3620 3630 3640 3650 3660 3670 3680 3690 3700 3710 3720 3730 3740 3750 3760 3770 3780 3790 3800 3810 3820 3830 3840 3850 3860 3870 3880 3890 3900 3910 3920 3930 3940 3950 3960 3970 3980 3990 4000 4010\n",
      "\n",
      "step 400\n",
      "Accuracy is:\n",
      "0.7264\n",
      " 4020 4030 4040 4050 4060 4070 4080 4090 4100 4110 4120 4130 4140 4150 4160 4170 4180 4190 4200 4210 4220 4230 4240 4250 4260 4270 4280 4290 4300 4310 4320 4330 4340 4350 4360 4370 4380 4390 4400 4410 4420 4430 4440 4450 4460 4470 4480 4490 4500 4510\n",
      "\n",
      "step 450\n",
      "Accuracy is:\n",
      "0.727\n",
      " 4520 4530 4540 4550 4560 4570 4580 4590 4600 4610 4620 4630 4640 4650 4660 4670 4680 4690 4700 4710 4720 4730 4740 4750 4760 4770 4780 4790 4800 4810 4820 4830 4840 4850 4860 4870 4880 4890 4900 4910 4920 4930 4940 4950 4960 4970 4980 4990 5000 5010\n",
      "\n",
      "step 500\n",
      "Accuracy is:\n",
      "0.7602\n",
      " 5020 5030 5040 5050 5060 5070 5080 5090 5100 5110 5120 5130 5140 5150 5160 5170 5180 5190 5200 5210 5220 5230 5240 5250 5260 5270 5280 5290 5300 5310 5320 5330 5340 5350 5360 5370 5380 5390 5400 5410 5420 5430 5440 5450 5460 5470 5480 5490 5500 5510\n",
      "\n",
      "step 550\n",
      "Accuracy is:\n",
      "0.7324\n",
      " 5520 5530 5540 5550 5560 5570 5580 5590 5600 5610 5620 5630 5640 5650 5660 5670 5680 5690 5700 5710 5720 5730 5740 5750 5760 5770 5780 5790 5800 5810 5820 5830 5840 5850 5860 5870 5880 5890 5900 5910 5920 5930 5940 5950 5960 5970 5980 5990 6000 6010\n",
      "\n",
      "step 600\n",
      "Accuracy is:\n",
      "0.7552\n",
      " 6020 6030 6040 6050 6060 6070 6080 6090 6100 6110 6120 6130 6140 6150 6160 6170 6180 6190 6200 6210 6220 6230 6240 6250 6260 6270 6280 6290 6300 6310 6320 6330 6340 6350 6360 6370 6380 6390 6400 6410 6420 6430 6440 6450 6460 6470 6480 6490 6500 6510\n",
      "\n",
      "step 650\n",
      "Accuracy is:\n",
      "0.7522\n",
      " 6520 6530 6540 6550 6560 6570 6580 6590 6600 6610 6620 6630 6640 6650 6660 6670 6680 6690 6700 6710 6720 6730 6740 6750 6760 6770 6780 6790 6800 6810 6820 6830 6840 6850 6860 6870 6880 6890 6900 6910 6920 6930 6940 6950 6960 6970 6980 6990 7000 7010\n",
      "\n",
      "step 700\n",
      "Accuracy is:\n",
      "0.7332\n",
      " 7020 7030 7040 7050 7060 7070 7080 7090 7100 7110 7120 7130 7140 7150 7160 7170 7180 7190 7200 7210 7220 7230 7240 7250 7260 7270 7280 7290 7300 7310 7320 7330 7340 7350 7360 7370 7380 7390 7400 7410 7420 7430 7440 7450 7460 7470 7480 7490 7500 7510\n",
      "\n",
      "step 750\n",
      "Accuracy is:\n",
      "0.7498\n",
      " 7520 7530 7540 7550 7560 7570 7580 7590 7600 7610 7620 7630 7640 7650 7660 7670 7680 7690 7700 7710 7720 7730 7740 7750 7760 7770 7780 7790 7800 7810 7820 7830 7840 7850 7860 7870 7880 7890 7900 7910 7920 7930 7940 7950 7960 7970 7980 7990 8000 8010\n",
      "\n",
      "step 800\n",
      "Accuracy is:\n",
      "0.7634\n",
      " 8020 8030 8040 8050 8060 8070 8080 8090 8100 8110 8120 8130 8140 8150 8160 8170 8180 8190 8200 8210 8220 8230 8240 8250 8260 8270 8280 8290 8300 8310 8320 8330 8340 8350 8360 8370 8380 8390 8400 8410 8420 8430 8440 8450 8460 8470 8480 8490 8500 8510\n",
      "\n",
      "step 850\n",
      "Accuracy is:\n",
      "0.776\n",
      " 8520 8530 8540 8550 8560 8570 8580 8590 8600 8610 8620 8630 8640 8650 8660 8670 8680 8690 8700 8710 8720 8730 8740 8750 8760 8770 8780 8790 8800 8810 8820 8830 8840 8850 8860 8870 8880 8890 8900 8910 8920 8930 8940 8950 8960 8970 8980 8990 9000 9010\n",
      "\n",
      "step 900\n",
      "Accuracy is:\n",
      "0.7546\n",
      " 9020 9030 9040 9050 9060 9070 9080 9090 9100 9110 9120 9130 9140 9150 9160 9170 9180 9190 9200 9210 9220 9230 9240 9250 9260 9270 9280 9290 9300 9310 9320 9330 9340 9350 9360 9370 9380 9390 9400 9410 9420 9430 9440 9450 9460 9470 9480 9490 9500 9510\n",
      "\n",
      "step 950\n",
      "Accuracy is:\n",
      "0.7686\n",
      " 9520 9530 9540 9550 9560 9570 9580 9590 9600 9610 9620 9630 9640 9650 9660 9670 9680 9690 9700 9710 9720 9730 9740 9750 9760 9770 9780 9790 9800 9810 9820 9830 9840 9850 9860 9870 9880 9890 9900 9910 9920 9930 9940 9950 9960 9970 9980 9990 10000 10010\n",
      "\n",
      "step 1000\n",
      "Accuracy is:\n",
      "0.7804\n",
      " 10020 10030 10040 10050 10060 10070 10080 10090 10100 10110 10120 10130 10140 10150 10160 10170 10180 10190 10200 10210 10220 10230 10240 10250 10260 10270 10280 10290 10300 10310 10320 10330 10340 10350 10360 10370 10380 10390 10400 10410 10420 10430 10440 10450 10460 10470 10480 10490 10500 10510\n",
      "\n",
      "step 1050\n",
      "Accuracy is:\n",
      "0.7706\n",
      " 10520 10530 10540 10550 10560 10570 10580 10590 10600 10610 10620 10630 10640 10650 10660 10670 10680 10690 10700 10710 10720 10730 10740 10750 10760 10770 10780 10790 10800 10810 10820 10830 10840 10850 10860 10870 10880 10890 10900 10910 10920 10930 10940 10950 10960 10970 10980 10990 11000 11010\n",
      "\n",
      "step 1100\n",
      "Accuracy is:\n",
      "0.7834\n",
      " 11020 11030 11040 11050 11060 11070 11080 11090 11100 11110 11120 11130 11140 11150 11160 11170 11180 11190 11200 11210 11220 11230 11240 11250 11260 11270 11280 11290 11300 11310 11320 11330 11340 11350 11360 11370 11380 11390 11400 11410 11420 11430 11440 11450 11460 11470 11480 11490 11500 11510\n",
      "\n",
      "step 1150\n",
      "Accuracy is:\n",
      "0.7742\n",
      " 11520 11530 11540 11550 11560 11570 11580 11590 11600 11610 11620 11630 11640 11650 11660 11670 11680 11690 11700 11710 11720 11730 11740 11750 11760 11770 11780 11790 11800 11810 11820 11830 11840 11850 11860 11870 11880 11890 11900 11910 11920 11930 11940 11950 11960 11970 11980 11990 12000 12010\n",
      "\n",
      "step 1200\n",
      "Accuracy is:\n",
      "0.793\n",
      " 12020 12030 12040 12050 12060 12070 12080 12090 12100 12110 12120 12130 12140 12150 12160 12170 12180 12190 12200 12210 12220 12230 12240 12250 12260 12270 12280 12290 12300 12310 12320 12330 12340 12350 12360 12370 12380 12390 12400 12410 12420 12430 12440 12450 12460 12470 12480 12490 12500 12510\n",
      "\n",
      "step 1250\n",
      "Accuracy is:\n",
      "0.7818\n",
      " 12520 12530 12540 12550 12560 12570 12580 12590 12600 12610 12620 12630 12640 12650 12660 12670 12680 12690 12700 12710 12720 12730 12740 12750 12760 12770 12780 12790 12800 12810 12820 12830 12840 12850 12860 12870 12880 12890 12900 12910 12920 12930 12940 12950 12960 12970 12980 12990 13000 13010\n",
      "\n",
      "step 1300\n",
      "Accuracy is:\n",
      "0.7806\n",
      " 13020 13030 13040 13050 13060 13070 13080 13090 13100 13110 13120 13130 13140 13150 13160 13170 13180 13190 13200 13210 13220 13230 13240 13250 13260 13270 13280 13290 13300 13310 13320 13330 13340 13350 13360 13370 13380 13390 13400 13410 13420 13430 13440 13450 13460 13470 13480 13490 13500 13510\n",
      "\n",
      "step 1350\n",
      "Accuracy is:\n",
      "0.764\n",
      " 13520 13530 13540 13550 13560 13570 13580 13590 13600 13610 13620 13630 13640 13650 13660 13670 13680 13690 13700 13710 13720 13730 13740 13750 13760 13770 13780 13790 13800 13810 13820 13830 13840 13850 13860 13870 13880 13890 13900 13910 13920 13930 13940 13950 13960 13970 13980 13990 14000 14010\n",
      "\n",
      "step 1400\n",
      "Accuracy is:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7798\n",
      " 14020 14030 14040 14050 14060 14070 14080 14090 14100 14110 14120 14130 14140 14150 14160 14170 14180 14190 14200 14210 14220 14230 14240 14250 14260 14270 14280 14290 14300 14310 14320 14330 14340 14350 14360 14370 14380 14390 14400 14410 14420 14430 14440 14450 14460 14470 14480 14490 14500 14510\n",
      "\n",
      "step 1450\n",
      "Accuracy is:\n",
      "0.7656\n",
      " 14520 14530 14540 14550 14560 14570 14580 14590 14600 14610 14620 14630 14640 14650 14660 14670 14680 14690 14700 14710 14720 14730 14740 14750 14760 14770 14780 14790 14800 14810 14820 14830 14840 14850 14860 14870 14880 14890 14900 14910 14920 14930 14940 14950 14960 14970 14980 14990 15000 15010\n",
      "\n",
      "step 1500\n",
      "Accuracy is:\n",
      "0.7626\n",
      " 15020 15030 15040 15050 15060 15070 15080 15090 15100 15110 15120 15130 15140 15150 15160 15170 15180 15190 15200 15210 15220 15230 15240 15250 15260 15270 15280 15290 15300 15310 15320 15330 15340 15350 15360 15370 15380 15390 15400 15410 15420 15430 15440 15450 15460 15470 15480 15490 15500 15510\n",
      "\n",
      "step 1550\n",
      "Accuracy is:\n",
      "0.7914\n",
      " 15520 15530 15540 15550 15560 15570 15580 15590 15600 15610 15620 15630 15640 15650 15660 15670 15680 15690 15700 15710 15720 15730 15740 15750 15760 15770 15780 15790 15800 15810 15820 15830 15840 15850 15860 15870 15880 15890 15900 15910 15920 15930 15940 15950 15960 15970 15980 15990 16000 16010\n",
      "\n",
      "step 1600\n",
      "Accuracy is:\n",
      "0.7856\n",
      " 16020 16030 16040 16050 16060 16070 16080 16090 16100 16110 16120 16130 16140 16150 16160 16170 16180 16190 16200 16210 16220 16230 16240 16250 16260 16270 16280 16290 16300 16310 16320 16330 16340 16350 16360 16370 16380 16390 16400 16410 16420 16430 16440 16450 16460 16470 16480 16490 16500 16510\n",
      "\n",
      "step 1650\n",
      "Accuracy is:\n",
      "0.7828\n",
      " 16520 16530 16540 16550 16560 16570 16580 16590 16600 16610 16620 16630 16640 16650 16660 16670 16680 16690 16700 16710 16720 16730 16740 16750 16760 16770 16780 16790 16800 16810 16820 16830 16840 16850 16860 16870 16880 16890 16900 16910 16920 16930 16940 16950 16960 16970 16980 16990 17000 17010\n",
      "\n",
      "step 1700\n",
      "Accuracy is:\n",
      "0.7706\n",
      " 17020 17030 17040 17050 17060 17070 17080 17090 17100 17110 17120 17130 17140 17150 17160 17170 17180 17190 17200 17210 17220 17230 17240 17250 17260 17270 17280 17290 17300 17310 17320 17330 17340 17350 17360 17370 17380 17390 17400 17410 17420 17430 17440 17450 17460 17470 17480 17490 17500 17510\n",
      "\n",
      "step 1750\n",
      "Accuracy is:\n",
      "0.7848\n",
      " 17520 17530 17540 17550 17560 17570 17580 17590 17600 17610 17620 17630 17640 17650 17660 17670 17680 17690 17700 17710 17720 17730 17740 17750 17760 17770 17780 17790 17800 17810 17820 17830 17840 17850 17860 17870 17880 17890 17900 17910 17920 17930 17940 17950 17960 17970 17980 17990 18000 18010\n",
      "\n",
      "step 1800\n",
      "Accuracy is:\n",
      "0.7854\n",
      " 18020 18030 18040 18050 18060 18070 18080 18090 18100 18110 18120 18130 18140 18150 18160 18170 18180 18190 18200 18210 18220 18230 18240 18250 18260 18270 18280 18290 18300 18310 18320 18330 18340 18350 18360 18370 18380 18390 18400 18410 18420 18430 18440 18450 18460 18470 18480 18490 18500 18510\n",
      "\n",
      "step 1850\n",
      "Accuracy is:\n",
      "0.7994\n",
      " 18520 18530 18540 18550 18560 18570 18580 18590 18600 18610 18620 18630 18640 18650 18660 18670 18680 18690 18700 18710 18720 18730 18740 18750 18760 18770 18780 18790 18800 18810 18820 18830 18840 18850 18860 18870 18880 18890 18900 18910 18920 18930 18940 18950 18960 18970 18980 18990 19000 19010\n",
      "\n",
      "step 1900\n",
      "Accuracy is:\n",
      "0.778\n",
      " 19020 19030 19040 19050 19060 19070 19080 19090 19100 19110 19120 19130 19140 19150 19160 19170 19180 19190 19200 19210 19220 19230 19240 19250 19260 19270 19280 19290 19300 19310 19320 19330 19340 19350 19360 19370 19380 19390 19400 19410 19420 19430 19440 19450 19460 19470 19480 19490 19500 19510\n",
      "\n",
      "step 1950\n",
      "Accuracy is:\n",
      "0.774\n",
      " 19520 19530 19540 19550 19560 19570 19580 19590 19600 19610 19620 19630 19640 19650 19660 19670 19680 19690 19700 19710 19720 19730 19740 19750 19760 19770 19780 19790 19800 19810 19820 19830 19840 19850 19860 19870 19880 19890 19900 19910 19920 19930 19940 19950 19960 19970 19980 19990 20000 20010\n",
      "\n",
      "step 2000\n",
      "Accuracy is:\n",
      "0.7786\n",
      " 20020 20030 20040 20050 20060 20070 20080 20090 20100 20110 20120 20130 20140 20150 20160 20170 20180 20190 20200 20210 20220 20230 20240 20250 20260 20270 20280 20290 20300 20310 20320 20330 20340 20350 20360 20370 20380 20390 20400 20410 20420 20430 20440 20450 20460 20470 20480 20490 20500 20510\n",
      "\n",
      "step 2050\n",
      "Accuracy is:\n",
      "0.8004\n",
      " 20520 20530 20540 20550 20560 20570 20580 20590 20600 20610 20620 20630 20640 20650 20660 20670 20680 20690 20700 20710 20720 20730 20740 20750 20760 20770 20780 20790 20800 20810 20820 20830 20840 20850 20860 20870 20880 20890 20900 20910 20920 20930 20940 20950 20960 20970 20980 20990 21000 21010\n",
      "\n",
      "step 2100\n",
      "Accuracy is:\n",
      "0.803\n",
      " 21020 21030 21040 21050 21060 21070 21080 21090 21100 21110 21120 21130 21140 21150 21160 21170 21180 21190 21200 21210 21220 21230 21240 21250 21260 21270 21280 21290 21300 21310 21320 21330 21340 21350 21360 21370 21380 21390 21400 21410 21420 21430 21440 21450 21460 21470 21480 21490 21500 21510\n",
      "\n",
      "step 2150\n",
      "Accuracy is:\n",
      "0.7946\n",
      " 21520 21530 21540 21550 21560 21570 21580 21590 21600 21610 21620 21630 21640 21650 21660 21670 21680 21690 21700 21710 21720 21730 21740 21750 21760 21770 21780 21790 21800 21810 21820 21830 21840 21850 21860 21870 21880 21890 21900 21910 21920 21930 21940 21950 21960 21970 21980 21990 22000 22010\n",
      "\n",
      "step 2200\n",
      "Accuracy is:\n",
      "0.7984\n",
      " 22020 22030 22040 22050 22060 22070 22080 22090 22100 22110 22120 22130 22140 22150 22160 22170 22180 22190 22200 22210 22220 22230 22240 22250 22260 22270 22280 22290 22300 22310 22320 22330 22340 22350 22360 22370 22380 22390 22400 22410 22420 22430 22440 22450 22460 22470 22480 22490 22500 22510\n",
      "\n",
      "step 2250\n",
      "Accuracy is:\n",
      "0.8056\n",
      " 22520 22530 22540 22550 22560 22570 22580 22590 22600 22610 22620 22630 22640 22650 22660 22670 22680 22690 22700 22710 22720 22730 22740 22750 22760 22770 22780 22790 22800 22810 22820 22830 22840 22850 22860 22870 22880 22890 22900 22910 22920 22930 22940 22950 22960 22970 22980 22990 23000 23010\n",
      "\n",
      "step 2300\n",
      "Accuracy is:\n",
      "0.813\n",
      " 23020 23030 23040 23050 23060 23070 23080 23090 23100 23110 23120 23130 23140 23150 23160 23170 23180 23190 23200 23210 23220 23230 23240 23250 23260 23270 23280 23290 23300 23310 23320 23330 23340 23350 23360 23370 23380 23390 23400 23410 23420 23430 23440 23450 23460 23470 23480 23490 23500 23510\n",
      "\n",
      "step 2350\n",
      "Accuracy is:\n",
      "0.7966\n",
      " 23520 23530 23540 23550 23560 23570 23580 23590 23600 23610 23620 23630 23640 23650 23660 23670 23680 23690 23700 23710 23720 23730 23740 23750 23760 23770 23780 23790 23800 23810 23820 23830 23840 23850 23860 23870 23880 23890 23900 23910 23920 23930 23940 23950 23960 23970 23980 23990 24000 24010\n",
      "\n",
      "step 2400\n",
      "Accuracy is:\n",
      "0.8112\n",
      " 24020 24030 24040 24050 24060 24070 24080 24090 24100 24110 24120 24130 24140 24150 24160 24170 24180 24190 24200 24210 24220 24230 24240 24250 24260 24270 24280 24290 24300 24310 24320 24330 24340 24350 24360 24370 24380 24390 24400 24410 24420 24430 24440 24450 24460 24470 24480 24490 24500 24510\n",
      "\n",
      "step 2450\n",
      "Accuracy is:\n",
      "0.7918\n",
      " 24520 24530 24540 24550 24560 24570 24580 24590 24600 24610 24620 24630 24640 24650 24660 24670 24680 24690 24700 24710 24720 24730 24740 24750 24760 24770 24780 24790 24800 24810 24820 24830 24840 24850 24860 24870 24880 24890 24900 24910 24920 24930 24940 24950 24960 24970 24980 24990 25000 25010\n",
      "\n",
      "step 2500\n",
      "Accuracy is:\n",
      "0.7842\n",
      " 25020 25030 25040 25050 25060 25070 25080 25090 25100 25110 25120 25130 25140 25150 25160 25170 25180 25190 25200 25210 25220 25230 25240 25250 25260 25270 25280 25290 25300 25310 25320 25330 25340 25350 25360 25370 25380 25390 25400 25410 25420 25430 25440 25450 25460 25470 25480 25490 25500 25510\n",
      "\n",
      "step 2550\n",
      "Accuracy is:\n",
      "0.8256\n",
      " 25520 25530 25540 25550 25560 25570 25580 25590 25600 25610 25620 25630 25640 25650 25660 25670 25680 25690 25700 25710 25720 25730 25740 25750 25760 25770 25780 25790 25800 25810 25820 25830 25840 25850 25860 25870 25880 25890 25900 25910 25920 25930 25940 25950 25960 25970 25980 25990 26000 26010\n",
      "\n",
      "step 2600\n",
      "Accuracy is:\n",
      "0.8054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 26020 26030 26040 26050 26060 26070 26080 26090 26100 26110 26120 26130 26140 26150 26160 26170 26180 26190 26200 26210 26220 26230 26240 26250 26260 26270 26280 26290 26300 26310 26320 26330 26340 26350 26360 26370 26380 26390 26400 26410 26420 26430 26440 26450 26460 26470 26480 26490 26500 26510\n",
      "\n",
      "step 2650\n",
      "Accuracy is:\n",
      "0.8102\n",
      " 26520 26530 26540 26550 26560 26570 26580 26590 26600 26610 26620 26630 26640 26650 26660 26670 26680 26690 26700 26710 26720 26730 26740 26750 26760 26770 26780 26790 26800 26810 26820 26830 26840 26850 26860 26870 26880 26890 26900 26910 26920 26930 26940 26950 26960 26970 26980 26990 27000 27010\n",
      "\n",
      "step 2700\n",
      "Accuracy is:\n",
      "0.7908\n",
      " 27020 27030 27040 27050 27060 27070 27080 27090 27100 27110 27120 27130 27140 27150 27160 27170 27180 27190 27200 27210 27220 27230 27240 27250 27260 27270 27280 27290 27300 27310 27320 27330 27340 27350 27360 27370 27380 27390 27400 27410 27420 27430 27440 27450 27460 27470 27480 27490 27500 27510\n",
      "\n",
      "step 2750\n",
      "Accuracy is:\n",
      "0.8122\n",
      " 27520 27530 27540 27550 27560 27570 27580 27590 27600 27610 27620 27630 27640 27650 27660 27670 27680 27690 27700 27710 27720 27730 27740 27750 27760 27770 27780 27790 27800 27810 27820 27830 27840 27850 27860 27870 27880 27890 27900 27910 27920 27930 27940 27950 27960 27970 27980 27990 28000 28010\n",
      "\n",
      "step 2800\n",
      "Accuracy is:\n",
      "0.8146\n",
      " 28020 28030 28040 28050 28060 28070 28080 28090 28100 28110 28120 28130 28140 28150 28160 28170 28180 28190 28200 28210 28220 28230 28240 28250 28260 28270 28280 28290 28300 28310 28320 28330 28340 28350 28360 28370 28380 28390 28400 28410 28420 28430 28440 28450 28460 28470 28480 28490 28500 28510\n",
      "\n",
      "step 2850\n",
      "Accuracy is:\n",
      "0.7984\n",
      " 28520 28530 28540 28550 28560 28570 28580 28590 28600 28610 28620 28630 28640 28650 28660 28670 28680 28690 28700 28710 28720 28730 28740 28750 28760 28770 28780 28790 28800 28810 28820 28830 28840 28850 28860 28870 28880 28890 28900 28910 28920 28930 28940 28950 28960 28970 28980 28990 29000 29010\n",
      "\n",
      "step 2900\n",
      "Accuracy is:\n",
      "0.7822\n",
      " 29020 29030 29040 29050 29060 29070 29080 29090 29100 29110 29120 29130 29140 29150 29160 29170 29180 29190 29200 29210 29220 29230 29240 29250 29260 29270 29280 29290 29300 29310 29320 29330 29340 29350 29360 29370 29380 29390 29400 29410 29420 29430 29440 29450 29460 29470 29480 29490 29500 29510\n",
      "\n",
      "step 2950\n",
      "Accuracy is:\n",
      "0.8044\n",
      " 29520 29530 29540 29550 29560 29570 29580 29590 29600 29610 29620 29630 29640 29650 29660 29670 29680 29690 29700 29710 29720 29730 29740 29750 29760 29770 29780 29790 29800 29810 29820 29830 29840 29850 29860 29870 29880 29890 29900 29910 29920 29930 29940 29950 29960 29970 29980 29990 30000\n",
      "\n",
      "FINAL Accuracy is:\n",
      "0.809\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    steps = 3000 #1850\n",
    "\n",
    "    print('INIT')\n",
    "    sess.run(init)\n",
    "\n",
    "    for j in range(steps):\n",
    "        # print('.', end='')\n",
    "        batch_x , batch_y = sih.next_batch(100) # Batch size -> 64 / 128\n",
    "        sess.run(train,feed_dict={x:batch_x,y_true:batch_y,hold_prob:0.5}) # initial:0.5 Hold prob -> 0.8 aprox\n",
    "\n",
    "        # PRINT OUT A MESSAGE EVERY 100 STEPS\n",
    "        if j%50 == 0:\n",
    "            print('\\n')\n",
    "            print('step {}'.format(j))\n",
    "            print('Accuracy is:')\n",
    "            # Test the Train Model\n",
    "            matches = tf.equal(tf.argmax(y_pred,1),tf.argmax(y_true,1))\n",
    "\n",
    "            acc = tf.reduce_mean(tf.cast(matches,tf.float32))\n",
    "\n",
    "            print(sess.run(acc,feed_dict={x:test_x,y_true:test_y,hold_prob:1.0}))\n",
    "\n",
    "\n",
    "    print('\\n')\n",
    "    print('FINAL Accuracy is:')\n",
    "    print(sess.run(acc,feed_dict={x:test_x,y_true:test_y,hold_prob:1.0}))\n",
    "    print('\\n')\n",
    "    \n",
    "    saver.save(sess, savefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./STORED_model/my_trained_model.json\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x121ee0240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classTypeId = 8 #values between 0..10\n",
    "imageNumber = 10 #value between 0..500\n",
    "with tf.Session() as sess:\n",
    "    # restore the model\n",
    "    saver.restore(sess, savefile)\n",
    "\n",
    "    myclass = sih.fileList[classTypeId]\n",
    "    evalImage = (np.load('./SKETCH_data/{}.npy'.format(myclass))[500 + imageNumber] / 255)\n",
    "\n",
    "    feed_dict = {x: np.reshape(evalImage,newshape=(1,784)), y_true: np.zeros((1, 10)), hold_prob : 0.5 }\n",
    "\n",
    "    classification = sess.run(tf.argmax(y_pred,1), feed_dict)\n",
    "\n",
    "    display(evalImage, myclass, sih.fileList[int(classification)])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Great Job!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
